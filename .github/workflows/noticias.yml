# Nombre del flujo de trabajo
name: Raspador de Noticias Diario

on:
  # Permite ejecutarlo manualmente desde la pestaña Actions de GitHub para probar
  workflow_dispatch:
  
  # Ejecución programada (cron job) para las 3 horas del día
  schedule:
    # Ejecución 1: A las 5:10 UTC (7:10 AM en España en verano)
    - cron: '10 5 * * *'
    
    # Ejecución 2: A las 11:00 UTC (1:00 PM en España en verano)
    - cron: '0 11 * * *'
    
    # Ejecución 3: A las 20:00 UTC (10:00 PM en España en verano)
    - cron: '0 20 * * *'

jobs:
  scrape-and-notify:
    # Se ejecutará en una máquina virtual con Ubuntu
    runs-on: ubuntu-latest

    steps:
      # 1. Descarga el código de tu repositorio
      - name: Checkout del código
        uses: actions/checkout@v4

      # 2. Configura el entorno de Python 3.10
      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # 3. Instala Google Chrome para Selenium
      - name: Instalar Google Chrome
        uses: browser-actions/setup-chrome@latest

      # 4. Instala las librerías del archivo requirements.txt
      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 5. Ejecuta el script principal
      - name: Ejecutar el script de scraping y notificación
        env:
          # Inyecta el "secret" que configuraste en GitHub
          NTFY_TOPIC: ${{ secrets.NTFY_TOPIC }}
        run: python main.py

      # 6. Sube los artefactos de depuración (capturas de pantalla) si se produce algún error
      - name: Subir capturas de pantalla de errores
        uses: actions/upload-artifact@v4
        with:
          name: debug-artifacts
          path: |
            *.png
            *.html
          if-no-files-found: ignore
